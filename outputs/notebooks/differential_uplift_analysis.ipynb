{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Uplift Analysis at Axial Seamount\n",
    "\n",
    "**Author:** Dax Soule  \n",
    "**Date:** January 2026  \n",
    "**Data Period:** 2015-01-01 to 2026-01-16\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook calculates **differential uplift** between two Bottom Pressure Recorders (NANO-BPRs) at Axial Seamount, an active submarine volcano on the Juan de Fuca Ridge. By comparing pressure measurements from two locations within the caldera, we can track volcanic deformation independently of regional oceanographic signals.\n",
    "\n",
    "### Scientific Background\n",
    "\n",
    "Axial Seamount is one of the most active submarine volcanoes in the world, with eruptions recorded in 1998, 2011, and 2015. Bottom Pressure Recorders (BPRs) measure the water column pressure above the seafloor, which changes as the seafloor moves up (inflation) or down (deflation) due to magma accumulation or withdrawal.\n",
    "\n",
    "**Differential uplift** (the difference between two BPRs) removes common-mode signals like:\n",
    "- Ocean tides\n",
    "- Atmospheric pressure changes\n",
    "- Regional oceanographic variability\n",
    "\n",
    "This leaves only the local volcanic deformation signal.\n",
    "\n",
    "### Stations\n",
    "\n",
    "| Station | Location | Description |\n",
    "|---------|----------|-------------|\n",
    "| MJ03E | Eastern Caldera | Reference station |\n",
    "| MJ03F | Central Caldera | Near the eruptive vent |\n",
    "\n",
    "The differential signal (MJ03E - MJ03F) shows relative vertical displacement between these two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment\n",
    "\n",
    "To reproduce this analysis, create a conda environment from the provided `environment.yml`:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate axial-bpr-analysis\n",
    "```\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Define data paths and analysis parameters. \n",
    "\n",
    "**Note:** You will need to update `MJ03E_PATH` and `MJ03F_PATH` to point to your local copies of the OOI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - UPDATE THESE FOR YOUR SYSTEM\n",
    "MJ03E_PATH = Path(\"/home/jovyan/ooi/kdata/RS03ECAL-MJ03E-06-BOTPTA302-streamed-botpt_nano_sample_15s\")\n",
    "MJ03F_PATH = Path(\"/home/jovyan/ooi/kdata/RS03CCAL-MJ03F-05-BOTPTA301-streamed-botpt_nano_sample_15s\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "\n",
    "# Time range for analysis\n",
    "TIME_START = \"2015-01-01\"\n",
    "TIME_END = \"2026-01-16\"\n",
    "TIME_START_YEAR = 2015\n",
    "TIME_END_YEAR = 2026\n",
    "\n",
    "print(f\"Analysis period: {TIME_START} to {TIME_END}\")\n",
    "print(f\"MJ03E data path: {MJ03E_PATH}\")\n",
    "print(f\"MJ03F data path: {MJ03F_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "### Pressure-to-Depth Conversion\n",
    "\n",
    "The BPR measures pressure in **psia** (pounds per square inch absolute). We convert this to depth in meters using a simplified linear relationship:\n",
    "\n",
    "$$\\text{depth}_m = (\\text{pressure}_{psia} - 14.7) \\times 0.670$$\n",
    "\n",
    "Where:\n",
    "- 14.7 psia is atmospheric pressure (subtracted to get gauge pressure)\n",
    "- 0.670 m/psi is an approximate conversion factor for seawater at this depth\n",
    "\n",
    "**Note:** This is an approximation. A more rigorous approach would account for salinity, temperature, and the equation of state for seawater. However, for differential measurements, these factors largely cancel out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_to_depth(pressure_psia):\n",
    "    \"\"\"Convert pressure in psia to depth in meters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pressure_psia : array-like\n",
    "        Pressure in pounds per square inch absolute\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        Depth in meters\n",
    "    \"\"\"\n",
    "    return (pressure_psia - 14.7) * 0.670"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Filtering\n",
    "\n",
    "The OOI data archive contains multiple file types. We specifically want the **15-second sample** files (`_15s_`) and only files that overlap with our target time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files_by_time_range(nc_files: list[Path]) -> list[Path]:\n",
    "    \"\"\"Filter NetCDF files to those covering the target time range.\n",
    "    \n",
    "    Only includes 15-second sample files that overlap with TIME_START to TIME_END.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_files : list of Path\n",
    "        List of NetCDF file paths\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of Path\n",
    "        Filtered and sorted list of file paths\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    # Pattern to extract start and end years from filename\n",
    "    pattern = re.compile(r\"_15s_(\\d{4})\\d{4}T\\d{6}-(\\d{4})\\d{4}T\")\n",
    "\n",
    "    for f in nc_files:\n",
    "        # Only include 15s sample files\n",
    "        if \"_15s_\" not in f.name:\n",
    "            continue\n",
    "\n",
    "        match = pattern.search(f.name)\n",
    "        if match:\n",
    "            start_year = int(match.group(1))\n",
    "            end_year = int(match.group(2))\n",
    "            # Include file if it overlaps with our time range\n",
    "            if start_year <= TIME_END_YEAR and end_year >= TIME_START_YEAR:\n",
    "                filtered.append(f)\n",
    "\n",
    "    return sorted(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Data Loading\n",
    "\n",
    "Each station's data is loaded file-by-file to manage memory (the full dataset is large). We:\n",
    "\n",
    "1. Load each NetCDF file\n",
    "2. Swap dimensions from `obs` to `time` for proper time-series handling\n",
    "3. Filter to our target time range\n",
    "4. Convert pressure to depth\n",
    "5. Resample from 15-second to **hourly** averages (reduces data volume by ~240x)\n",
    "6. Concatenate all chunks and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station(data_path: Path, station_name: str) -> pd.Series:\n",
    "    \"\"\"Load pressure data for a station, filtered to time range, resampled to hourly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        Directory containing NetCDF files for this station\n",
    "    station_name : str\n",
    "        Name of station (for logging)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Hourly depth time series with DatetimeIndex\n",
    "    \"\"\"\n",
    "    all_files = sorted(data_path.glob(\"*.nc\"))\n",
    "    nc_files = filter_files_by_time_range(all_files)\n",
    "\n",
    "    print(f\"{station_name}: Loading {len(nc_files)} files\")\n",
    "\n",
    "    hourly_chunks = []\n",
    "\n",
    "    for i, f in enumerate(nc_files):\n",
    "        if i % 50 == 0:  # Progress update every 50 files\n",
    "            print(f\"  Processing file {i+1}/{len(nc_files)}...\")\n",
    "\n",
    "        # Load single file\n",
    "        ds = xr.open_dataset(f, engine=\"netcdf4\")\n",
    "        \n",
    "        # OOI data uses 'obs' as the dimension; swap to 'time' for time-series operations\n",
    "        ds = ds.swap_dims({\"obs\": \"time\"})\n",
    "\n",
    "        # Filter to target time range\n",
    "        ds = ds.sel(time=slice(TIME_START, TIME_END))\n",
    "\n",
    "        if len(ds.time) == 0:\n",
    "            ds.close()\n",
    "            continue\n",
    "\n",
    "        # Get pressure, convert to depth, resample to hourly (reduces memory)\n",
    "        pressure = ds[\"bottom_pressure\"].values\n",
    "        time = ds[\"time\"].values\n",
    "        ds.close()\n",
    "\n",
    "        # Create series and resample\n",
    "        depth = pressure_to_depth(pressure)\n",
    "        series = pd.Series(depth, index=pd.DatetimeIndex(time))\n",
    "        hourly = series.resample(\"1h\").mean()\n",
    "        hourly_chunks.append(hourly)\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    result = pd.concat(hourly_chunks).sort_index()\n",
    "    result = result[~result.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    print(f\"{station_name}: {len(result)} hourly observations\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Both Stations\n",
    "\n",
    "**Warning:** This cell takes several minutes to run as it processes hundreds of NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MJ03E (Eastern Caldera)...\")\n",
    "depth_e = load_station(MJ03E_PATH, \"MJ03E\")\n",
    "\n",
    "print(\"\\nLoading MJ03F (Central Caldera)...\")\n",
    "depth_f = load_station(MJ03F_PATH, \"MJ03F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "print(\"MJ03E depth range:\", depth_e.min(), \"to\", depth_e.max(), \"meters\")\n",
    "print(\"MJ03F depth range:\", depth_f.min(), \"to\", depth_f.max(), \"meters\")\n",
    "print(\"\\nTime range:\", depth_e.index.min(), \"to\", depth_e.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quality Control: Spike Removal\n",
    "\n",
    "### The Problem\n",
    "\n",
    "BPR data contains occasional **spikes** - brief, anomalous readings caused by sensor glitches, electrical interference, or data transmission errors. These appear as sudden jumps that don't represent real seafloor motion.\n",
    "\n",
    "### Why MAD Instead of Standard Deviation?\n",
    "\n",
    "A common approach is to flag values more than 3 standard deviations from a rolling mean. However, **standard deviation is sensitive to outliers** - the very spikes we're trying to detect inflate the standard deviation, making them harder to catch.\n",
    "\n",
    "**Median Absolute Deviation (MAD)** is a robust alternative:\n",
    "\n",
    "$$\\text{MAD} = \\text{median}(|X_i - \\text{median}(X)|)$$\n",
    "\n",
    "Because MAD uses medians instead of means, outliers have minimal influence on the threshold calculation.\n",
    "\n",
    "### Scaling Factor\n",
    "\n",
    "For normally distributed data, the relationship between standard deviation and MAD is:\n",
    "\n",
    "$$\\sigma \\approx 1.4826 \\times \\text{MAD}$$\n",
    "\n",
    "We use this scaling factor so our threshold is interpretable in terms of \"sigma\" equivalents.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Calculate a **24-hour centered rolling median** (robust central tendency)\n",
    "2. Compute the absolute deviation of each point from this rolling median\n",
    "3. Calculate a **rolling MAD** from these deviations\n",
    "4. Scale MAD by 1.4826 to approximate standard deviation\n",
    "5. Flag points where deviation > threshold × scaled_MAD\n",
    "6. Replace flagged points with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spikes(series: pd.Series, window_hours: int = 24, threshold: float = 5.0) -> pd.Series:\n",
    "    \"\"\"Remove spikes using rolling median and MAD (median absolute deviation).\n",
    "\n",
    "    MAD is more robust to outliers than standard deviation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Hourly depth time series\n",
    "    window_hours : int\n",
    "        Rolling window size in hours (default: 24)\n",
    "    threshold : float\n",
    "        Number of scaled MADs for spike threshold (default: 5.0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series with spikes replaced by NaN\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "\n",
    "    # Use median (robust to outliers)\n",
    "    rolling_median = cleaned.rolling(window=window_hours, center=True, min_periods=1).median()\n",
    "\n",
    "    # Calculate MAD (median absolute deviation) - more robust than std\n",
    "    deviation = (cleaned - rolling_median).abs()\n",
    "    rolling_mad = deviation.rolling(window=window_hours, center=True, min_periods=1).median()\n",
    "\n",
    "    # Scale MAD to be comparable to std (for normal distribution, std ≈ 1.4826 * MAD)\n",
    "    scaled_mad = 1.4826 * rolling_mad\n",
    "\n",
    "    # Flag values more than threshold MADs from rolling median\n",
    "    is_spike = deviation > (threshold * scaled_mad)\n",
    "\n",
    "    n_spikes = is_spike.sum()\n",
    "    if n_spikes > 0:\n",
    "        print(f\"    Removed {n_spikes} spikes ({100*n_spikes/len(series):.2f}%)\")\n",
    "        cleaned[is_spike] = pd.NA\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Spike Removal to Individual Stations\n",
    "\n",
    "We use a **conservative threshold of 5.0** for individual station data to catch only obvious sensor glitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing spikes from MJ03E...\")\n",
    "depth_e_clean = remove_spikes(depth_e, window_hours=24, threshold=5.0)\n",
    "\n",
    "print(\"Removing spikes from MJ03F...\")\n",
    "depth_f_clean = remove_spikes(depth_f, window_hours=24, threshold=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing Differential Uplift\n",
    "\n",
    "### Alignment and Calculation\n",
    "\n",
    "The two stations don't always have simultaneous measurements (due to data gaps, maintenance, etc.). We:\n",
    "\n",
    "1. Combine both series into a DataFrame\n",
    "2. Drop rows where either station is missing\n",
    "3. Calculate the difference: **MJ03E - MJ03F**\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Positive differential** = MJ03E is deeper than MJ03F = Central Caldera (MJ03F) has uplifted relative to Eastern Caldera\n",
    "- **Negative differential** = MJ03E is shallower = Central Caldera has subsided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align on common time index\n",
    "combined = pd.DataFrame({\n",
    "    \"depth_mj03e_m\": depth_e_clean,\n",
    "    \"depth_mj03f_m\": depth_f_clean\n",
    "}).dropna()\n",
    "\n",
    "# Calculate differential depth: MJ03E - MJ03F\n",
    "combined[\"differential_m\"] = combined[\"depth_mj03e_m\"] - combined[\"depth_mj03f_m\"]\n",
    "\n",
    "print(f\"Combined dataset: {len(combined)} hourly observations\")\n",
    "print(f\"Time range: {combined.index.min()} to {combined.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Removal on Differential Signal\n",
    "\n",
    "Some spikes only appear in the **differential** signal - when one sensor glitches but not the other. These aren't caught by filtering individual stations.\n",
    "\n",
    "We use a **more aggressive threshold of 3.5** for the differential signal because:\n",
    "1. The differential is inherently smoother (common-mode noise removed)\n",
    "2. Spikes in the differential are more likely to be artifacts\n",
    "3. We want to preserve real volcanic signals while removing obvious glitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing spikes from differential signal...\")\n",
    "combined[\"differential_m\"] = remove_spikes(combined[\"differential_m\"], window_hours=24, threshold=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Averaging\n",
    "\n",
    "For visualization and analysis, we resample to **daily means**. This:\n",
    "- Removes residual tidal signals\n",
    "- Reduces noise\n",
    "- Makes long-term trends clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = combined.resample(\"1D\").mean()\n",
    "print(f\"Daily dataset: {len(daily)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "### Individual Station Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# MJ03E\n",
    "axes[0].plot(daily.index, daily[\"depth_mj03e_m\"], color=\"blue\", linewidth=0.5)\n",
    "axes[0].set_ylabel(\"Depth (m)\")\n",
    "axes[0].set_title(\"MJ03E - Eastern Caldera\")\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MJ03F\n",
    "axes[1].plot(daily.index, daily[\"depth_mj03f_m\"], color=\"red\", linewidth=0.5)\n",
    "axes[1].set_ylabel(\"Depth (m)\")\n",
    "axes[1].set_title(\"MJ03F - Central Caldera\")\n",
    "axes[1].set_xlabel(\"Year\")\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Uplift Time Series\n",
    "\n",
    "This is the key result showing volcanic deformation at Axial Seamount.\n",
    "\n",
    "**Reference lines:**\n",
    "- **Solid red**: The 2015 pre-eruption high (maximum differential before the April 2015 eruption)\n",
    "- **Dashed red**: ±20 cm from the 2015 high (uncertainty/forecasting bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 2015 high value for reference line\n",
    "uplift_2015 = daily[\"differential_m\"][\"2015\"]\n",
    "high_2015 = uplift_2015.max()\n",
    "print(f\"2015 pre-eruption high: {high_2015:.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality figure\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Plot the data\n",
    "ax.plot(daily.index, daily[\"differential_m\"].values,\n",
    "        color=\"#2E86AB\", linewidth=1, label=\"Daily mean\")\n",
    "\n",
    "# Add red horizontal line for 2015 high\n",
    "ax.axhline(y=high_2015, color=\"red\", linestyle=\"-\", linewidth=1.5,\n",
    "           label=f\"2015 high ({high_2015:.2f} m)\")\n",
    "\n",
    "# Add red dashed lines at +/- 20 cm from 2015 high\n",
    "ax.axhline(y=high_2015 + 0.20, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "ax.axhline(y=high_2015 - 0.20, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Differential Depth (m)\")\n",
    "ax.set_title(\"Differential Uplift at Axial Seamount (MJ03E − MJ03F)\")\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "# Clean up\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\n",
    "ax.legend(loc=\"lower right\", framealpha=0.9, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reset rcParams\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The plot shows:\n",
    "\n",
    "1. **Early 2015**: Gradual inflation as magma accumulates beneath the caldera\n",
    "2. **April 2015**: Sharp drop (~1.3 m) marking the eruption and magma withdrawal\n",
    "3. **2015-2026**: Steady re-inflation as the magma chamber recharges\n",
    "4. **~2022**: The differential crossed the 2015 pre-eruption high\n",
    "5. **Present (2026)**: Now ~20 cm above the 2015 high, suggesting continued inflation\n",
    "\n",
    "This pattern is consistent with the volcanic cycle at Axial Seamount: gradual inflation between eruptions, followed by rapid deflation during eruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Export\n",
    "\n",
    "Export the cleaned data to Parquet format for use in other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export hourly data\n",
    "combined.to_parquet(OUTPUT_DIR / \"differential_uplift_hourly.parquet\")\n",
    "print(f\"Exported: differential_uplift_hourly.parquet ({len(combined)} rows)\")\n",
    "\n",
    "# Export daily data\n",
    "daily.to_parquet(OUTPUT_DIR / \"differential_uplift_daily.parquet\")\n",
    "print(f\"Exported: differential_uplift_daily.parquet ({len(daily)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Exported Data\n",
    "\n",
    "To use this data in another analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: loading and joining with another dataset\n",
    "bpr = pd.read_parquet(OUTPUT_DIR / \"differential_uplift_daily.parquet\")\n",
    "print(bpr.head())\n",
    "print(f\"\\nColumns: {list(bpr.columns)}\")\n",
    "print(f\"Index: {bpr.index.name} ({bpr.index.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data loading** from OOI NetCDF archives with memory-efficient chunked processing\n",
    "2. **Pressure-to-depth conversion** using a simplified linear relationship\n",
    "3. **Quality control** using MAD-based spike detection (more robust than standard deviation)\n",
    "4. **Differential uplift calculation** to isolate volcanic deformation from oceanographic signals\n",
    "5. **Visualization** of the ~11-year volcanic inflation/deflation cycle\n",
    "6. **Data export** to Parquet for cross-instrument analysis\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Axial Seamount has been steadily inflating since the April 2015 eruption\n",
    "- As of early 2026, the differential uplift has exceeded the 2015 pre-eruption level by ~20 cm\n",
    "- The inflation rate appears roughly linear, suggesting continued magma recharge\n",
    "\n",
    "### References\n",
    "\n",
    "- Nooner, S. L., & Chadwick, W. W. (2016). Inflation-predictable behavior and co-eruption deformation at Axial Seamount. *Science*, 354(6318), 1399-1403.\n",
    "- OOI Cabled Array: https://oceanobservatories.org/array/cabled-array/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
