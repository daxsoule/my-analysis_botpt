{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Differential Uplift Analysis at Axial Seamount\n\n**Author:** Dax Soule  \n**Date:** January 2026  \n**Data Period:** 2015-01-01 to 2026-01-16\n\n---\n\n## Overview\n\nThis notebook calculates **differential uplift** between two Bottom Pressure Recorders (NANO-BPRs) at Axial Seamount, an active submarine volcano on the Juan de Fuca Ridge. By comparing pressure measurements from two locations within the caldera, we can track volcanic deformation independently of regional oceanographic signals.\n\n### Scientific Background\n\nAxial Seamount is one of the most active submarine volcanoes in the world, with eruptions recorded in 1998, 2011, and 2015. Bottom Pressure Recorders (BPRs) measure the water column pressure above the seafloor, which changes as the seafloor moves up (inflation) or down (deflation) due to magma accumulation or withdrawal.\n\n**Differential uplift** (the difference between two BPRs) removes common-mode signals like:\n- Ocean tides\n- Atmospheric pressure changes\n- Regional oceanographic variability\n\nThis leaves only the local volcanic deformation signal.\n\n### Stations\n\n| Station | Location | Description |\n|---------|----------|-------------|\n| MJ03E | Eastern Caldera | Reference station (lower uplift) |\n| MJ03F | Central Caldera | Near the eruptive vent (maximum uplift) |\n\nThe differential signal (MJ03F - MJ03E) shows relative vertical displacement between these two points. This convention (F minus E) matches the standard used by the Axial research team, where positive/increasing values indicate uplift at the caldera center relative to the eastern rim."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment\n",
    "\n",
    "To reproduce this analysis, create a conda environment from the provided `environment.yml`:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate axial-bpr-analysis\n",
    "```\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Define data paths and analysis parameters. \n",
    "\n",
    "**Note:** You will need to update `MJ03E_PATH` and `MJ03F_PATH` to point to your local copies of the OOI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - UPDATE THESE FOR YOUR SYSTEM\n",
    "MJ03E_PATH = Path(\"/home/jovyan/ooi/kdata/RS03ECAL-MJ03E-06-BOTPTA302-streamed-botpt_nano_sample_15s\")\n",
    "MJ03F_PATH = Path(\"/home/jovyan/ooi/kdata/RS03CCAL-MJ03F-05-BOTPTA301-streamed-botpt_nano_sample_15s\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "\n",
    "# Time range for analysis\n",
    "TIME_START = \"2015-01-01\"\n",
    "TIME_END = \"2026-01-16\"\n",
    "TIME_START_YEAR = 2015\n",
    "TIME_END_YEAR = 2026\n",
    "\n",
    "print(f\"Analysis period: {TIME_START} to {TIME_END}\")\n",
    "print(f\"MJ03E data path: {MJ03E_PATH}\")\n",
    "print(f\"MJ03F data path: {MJ03F_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "### Pressure-to-Depth Conversion\n",
    "\n",
    "The BPR measures pressure in **psia** (pounds per square inch absolute). We convert this to depth in meters using a simplified linear relationship:\n",
    "\n",
    "$$\\text{depth}_m = (\\text{pressure}_{psia} - 14.7) \\times 0.670$$\n",
    "\n",
    "Where:\n",
    "- 14.7 psia is atmospheric pressure (subtracted to get gauge pressure)\n",
    "- 0.670 m/psi is an approximate conversion factor for seawater at this depth\n",
    "\n",
    "**Note:** This is an approximation. A more rigorous approach would account for salinity, temperature, and the equation of state for seawater. However, for differential measurements, these factors largely cancel out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_to_depth(pressure_psia):\n",
    "    \"\"\"Convert pressure in psia to depth in meters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pressure_psia : array-like\n",
    "        Pressure in pounds per square inch absolute\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        Depth in meters\n",
    "    \"\"\"\n",
    "    return (pressure_psia - 14.7) * 0.670"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Filtering\n",
    "\n",
    "The OOI data archive contains multiple file types. We specifically want the **15-second sample** files (`_15s_`) and only files that overlap with our target time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files_by_time_range(nc_files: list[Path]) -> list[Path]:\n",
    "    \"\"\"Filter NetCDF files to those covering the target time range.\n",
    "    \n",
    "    Only includes 15-second sample files that overlap with TIME_START to TIME_END.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_files : list of Path\n",
    "        List of NetCDF file paths\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of Path\n",
    "        Filtered and sorted list of file paths\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    # Pattern to extract start and end years from filename\n",
    "    pattern = re.compile(r\"_15s_(\\d{4})\\d{4}T\\d{6}-(\\d{4})\\d{4}T\")\n",
    "\n",
    "    for f in nc_files:\n",
    "        # Only include 15s sample files\n",
    "        if \"_15s_\" not in f.name:\n",
    "            continue\n",
    "\n",
    "        match = pattern.search(f.name)\n",
    "        if match:\n",
    "            start_year = int(match.group(1))\n",
    "            end_year = int(match.group(2))\n",
    "            # Include file if it overlaps with our time range\n",
    "            if start_year <= TIME_END_YEAR and end_year >= TIME_START_YEAR:\n",
    "                filtered.append(f)\n",
    "\n",
    "    return sorted(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Data Loading\n",
    "\n",
    "Each station's data is loaded file-by-file to manage memory (the full dataset is large). We:\n",
    "\n",
    "1. Load each NetCDF file\n",
    "2. Swap dimensions from `obs` to `time` for proper time-series handling\n",
    "3. Filter to our target time range\n",
    "4. Convert pressure to depth\n",
    "5. Resample from 15-second to **hourly** averages (reduces data volume by ~240x)\n",
    "6. Concatenate all chunks and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station(data_path: Path, station_name: str) -> pd.Series:\n",
    "    \"\"\"Load pressure data for a station, filtered to time range, resampled to hourly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        Directory containing NetCDF files for this station\n",
    "    station_name : str\n",
    "        Name of station (for logging)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Hourly depth time series with DatetimeIndex\n",
    "    \"\"\"\n",
    "    all_files = sorted(data_path.glob(\"*.nc\"))\n",
    "    nc_files = filter_files_by_time_range(all_files)\n",
    "\n",
    "    print(f\"{station_name}: Loading {len(nc_files)} files\")\n",
    "\n",
    "    hourly_chunks = []\n",
    "\n",
    "    for i, f in enumerate(nc_files):\n",
    "        if i % 50 == 0:  # Progress update every 50 files\n",
    "            print(f\"  Processing file {i+1}/{len(nc_files)}...\")\n",
    "\n",
    "        # Load single file\n",
    "        ds = xr.open_dataset(f, engine=\"netcdf4\")\n",
    "        \n",
    "        # OOI data uses 'obs' as the dimension; swap to 'time' for time-series operations\n",
    "        ds = ds.swap_dims({\"obs\": \"time\"})\n",
    "\n",
    "        # Filter to target time range\n",
    "        ds = ds.sel(time=slice(TIME_START, TIME_END))\n",
    "\n",
    "        if len(ds.time) == 0:\n",
    "            ds.close()\n",
    "            continue\n",
    "\n",
    "        # Get pressure, convert to depth, resample to hourly (reduces memory)\n",
    "        pressure = ds[\"bottom_pressure\"].values\n",
    "        time = ds[\"time\"].values\n",
    "        ds.close()\n",
    "\n",
    "        # Create series and resample\n",
    "        depth = pressure_to_depth(pressure)\n",
    "        series = pd.Series(depth, index=pd.DatetimeIndex(time))\n",
    "        hourly = series.resample(\"1h\").mean()\n",
    "        hourly_chunks.append(hourly)\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    result = pd.concat(hourly_chunks).sort_index()\n",
    "    result = result[~result.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    print(f\"{station_name}: {len(result)} hourly observations\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Both Stations\n",
    "\n",
    "**Warning:** This cell takes several minutes to run as it processes hundreds of NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MJ03E (Eastern Caldera)...\")\n",
    "depth_e = load_station(MJ03E_PATH, \"MJ03E\")\n",
    "\n",
    "print(\"\\nLoading MJ03F (Central Caldera)...\")\n",
    "depth_f = load_station(MJ03F_PATH, \"MJ03F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "print(\"MJ03E depth range:\", depth_e.min(), \"to\", depth_e.max(), \"meters\")\n",
    "print(\"MJ03F depth range:\", depth_f.min(), \"to\", depth_f.max(), \"meters\")\n",
    "print(\"\\nTime range:\", depth_e.index.min(), \"to\", depth_e.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quality Control: Spike Removal\n",
    "\n",
    "### The Problem\n",
    "\n",
    "BPR data contains occasional **spikes** - brief, anomalous readings caused by sensor glitches, electrical interference, or data transmission errors. These appear as sudden jumps that don't represent real seafloor motion.\n",
    "\n",
    "### Why MAD Instead of Standard Deviation?\n",
    "\n",
    "A common approach is to flag values more than 3 standard deviations from a rolling mean. However, **standard deviation is sensitive to outliers** - the very spikes we're trying to detect inflate the standard deviation, making them harder to catch.\n",
    "\n",
    "**Median Absolute Deviation (MAD)** is a robust alternative:\n",
    "\n",
    "$$\\text{MAD} = \\text{median}(|X_i - \\text{median}(X)|)$$\n",
    "\n",
    "Because MAD uses medians instead of means, outliers have minimal influence on the threshold calculation.\n",
    "\n",
    "### Scaling Factor\n",
    "\n",
    "For normally distributed data, the relationship between standard deviation and MAD is:\n",
    "\n",
    "$$\\sigma \\approx 1.4826 \\times \\text{MAD}$$\n",
    "\n",
    "We use this scaling factor so our threshold is interpretable in terms of \"sigma\" equivalents.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Calculate a **24-hour centered rolling median** (robust central tendency)\n",
    "2. Compute the absolute deviation of each point from this rolling median\n",
    "3. Calculate a **rolling MAD** from these deviations\n",
    "4. Scale MAD by 1.4826 to approximate standard deviation\n",
    "5. Flag points where deviation > threshold × scaled_MAD\n",
    "6. Replace flagged points with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spikes(series: pd.Series, window_hours: int = 24, threshold: float = 5.0) -> pd.Series:\n",
    "    \"\"\"Remove spikes using rolling median and MAD (median absolute deviation).\n",
    "\n",
    "    MAD is more robust to outliers than standard deviation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Hourly depth time series\n",
    "    window_hours : int\n",
    "        Rolling window size in hours (default: 24)\n",
    "    threshold : float\n",
    "        Number of scaled MADs for spike threshold (default: 5.0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series with spikes replaced by NaN\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "\n",
    "    # Use median (robust to outliers)\n",
    "    rolling_median = cleaned.rolling(window=window_hours, center=True, min_periods=1).median()\n",
    "\n",
    "    # Calculate MAD (median absolute deviation) - more robust than std\n",
    "    deviation = (cleaned - rolling_median).abs()\n",
    "    rolling_mad = deviation.rolling(window=window_hours, center=True, min_periods=1).median()\n",
    "\n",
    "    # Scale MAD to be comparable to std (for normal distribution, std ≈ 1.4826 * MAD)\n",
    "    scaled_mad = 1.4826 * rolling_mad\n",
    "\n",
    "    # Flag values more than threshold MADs from rolling median\n",
    "    is_spike = deviation > (threshold * scaled_mad)\n",
    "\n",
    "    n_spikes = is_spike.sum()\n",
    "    if n_spikes > 0:\n",
    "        print(f\"    Removed {n_spikes} spikes ({100*n_spikes/len(series):.2f}%)\")\n",
    "        cleaned[is_spike] = pd.NA\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Spike Removal to Individual Stations\n",
    "\n",
    "We use a **conservative threshold of 5.0** for individual station data to catch only obvious sensor glitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing spikes from MJ03E...\")\n",
    "depth_e_clean = remove_spikes(depth_e, window_hours=24, threshold=5.0)\n",
    "\n",
    "print(\"Removing spikes from MJ03F...\")\n",
    "depth_f_clean = remove_spikes(depth_f, window_hours=24, threshold=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Computing Differential Uplift\n\n### Alignment and Calculation\n\nThe two stations don't always have simultaneous measurements (due to data gaps, maintenance, etc.). We:\n\n1. Combine both series into a DataFrame\n2. Drop rows where either station is missing\n3. Calculate the difference: **MJ03F - MJ03E** (following the Axial team convention)\n\n### Interpretation\n\nUsing the F - E convention (standard for Axial Seamount research):\n- **Increasing differential** = Central Caldera (MJ03F) is uplifting relative to Eastern Caldera = Inflation\n- **Decreasing differential** = Central Caldera has subsided relative to Eastern Caldera = Deflation\n\nThis convention makes the signal intuitive: upward trends indicate volcanic inflation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Align on common time index\ncombined = pd.DataFrame({\n    \"depth_mj03e_m\": depth_e_clean,\n    \"depth_mj03f_m\": depth_f_clean\n}).dropna()\n\n# Calculate differential depth: MJ03F - MJ03E (Axial team convention)\n# Positive values = center is shallower (uplifted) relative to east\ncombined[\"differential_m\"] = combined[\"depth_mj03f_m\"] - combined[\"depth_mj03e_m\"]\n\nprint(f\"Combined dataset: {len(combined)} hourly observations\")\nprint(f\"Time range: {combined.index.min()} to {combined.index.max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Removal on Differential Signal\n",
    "\n",
    "Some spikes only appear in the **differential** signal - when one sensor glitches but not the other. These aren't caught by filtering individual stations.\n",
    "\n",
    "We use a **more aggressive threshold of 3.5** for the differential signal because:\n",
    "1. The differential is inherently smoother (common-mode noise removed)\n",
    "2. Spikes in the differential are more likely to be artifacts\n",
    "3. We want to preserve real volcanic signals while removing obvious glitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing spikes from differential signal...\")\n",
    "combined[\"differential_m\"] = remove_spikes(combined[\"differential_m\"], window_hours=24, threshold=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Averaging\n",
    "\n",
    "For visualization and analysis, we resample to **daily means**. This:\n",
    "- Removes residual tidal signals\n",
    "- Reduces noise\n",
    "- Makes long-term trends clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = combined.resample(\"1D\").mean()\n",
    "print(f\"Daily dataset: {len(daily)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "### Individual Station Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# MJ03E\n",
    "axes[0].plot(daily.index, daily[\"depth_mj03e_m\"], color=\"blue\", linewidth=0.5)\n",
    "axes[0].set_ylabel(\"Depth (m)\")\n",
    "axes[0].set_title(\"MJ03E - Eastern Caldera\")\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MJ03F\n",
    "axes[1].plot(daily.index, daily[\"depth_mj03f_m\"], color=\"red\", linewidth=0.5)\n",
    "axes[1].set_ylabel(\"Depth (m)\")\n",
    "axes[1].set_title(\"MJ03F - Central Caldera\")\n",
    "axes[1].set_xlabel(\"Year\")\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Differential Uplift Time Series\n\nThis is the key result showing volcanic deformation at Axial Seamount.\n\n**Reference lines:**\n- **Solid red**: The 2015 pre-eruption high (maximum differential before the April 2015 eruption)\n- **Dashed red**: ±30 cm from the 2015 high, reflecting the observation that the 2015 eruption threshold was ~30 cm higher than the 2011 threshold (Nooner & Chadwick, 2016)\n\n**Note on eruption thresholds:** While Axial exhibits \"inflation-predictable\" behavior, the exact threshold can vary between eruption cycles. The 2015 threshold was elevated relative to 2011, and future thresholds may also differ. As the Axial team notes: \"the pattern could change.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find 2015 pre-eruption high value for reference line\n# With F-E convention, the pre-eruption peak is the MAXIMUM before the April eruption\nuplift_2015_pre_eruption = daily[\"differential_m\"][\"2015-01-01\":\"2015-04-24\"]\nhigh_2015 = uplift_2015_pre_eruption.max()\n\n# Find the post-eruption low to calculate deflation magnitude\nuplift_2015_post_eruption = daily[\"differential_m\"][\"2015-04-24\":\"2015-06-01\"]\nlow_2015 = uplift_2015_post_eruption.min()\ndeflation_magnitude = high_2015 - low_2015\n\nprint(f\"2015 pre-eruption high: {high_2015:.2f} m\")\nprint(f\"2015 post-eruption low: {low_2015:.2f} m\")\nprint(f\"Differential deflation: {deflation_magnitude:.2f} m\")\nprint(f\"(Note: Total deflation at MJ03F was ~2.4 m; differential is smaller because MJ03E also deflects)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Publication-quality figure\nplt.rcParams.update({\n    'font.size': 10,\n    'axes.labelsize': 11,\n    'axes.titlesize': 12,\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n})\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Plot the data\nax.plot(daily.index, daily[\"differential_m\"].values,\n        color=\"#2E86AB\", linewidth=1, label=\"Daily mean\")\n\n# Add reference line for 2015 pre-eruption high\nax.axhline(y=high_2015, color=\"red\", linestyle=\"-\", linewidth=1.5,\n           label=f\"2015 eruption threshold ({high_2015:.2f} m)\")\n\n# Add threshold uncertainty bands (±30 cm, based on 2011 vs 2015 threshold difference)\nax.axhline(y=high_2015 + 0.30, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.7)\nax.axhline(y=high_2015 - 0.30, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.7,\n           label=\"Threshold uncertainty (±30 cm)\")\n\n# Add annotation for the 2015 eruption\neruption_date = pd.Timestamp(\"2015-04-24\")\nax.annotate(\"2015 Eruption\\n(Apr 24)\", \n            xy=(eruption_date, low_2015), \n            xytext=(pd.Timestamp(\"2016-06-01\"), low_2015 + 0.3),\n            fontsize=9, ha='left',\n            arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n\n# Add annotation for deflation magnitude\nax.annotate(f\"Deflation: {deflation_magnitude:.2f} m\\n(~2.4 m total at MJ03F)\", \n            xy=(pd.Timestamp(\"2015-06-01\"), (high_2015 + low_2015)/2),\n            xytext=(pd.Timestamp(\"2016-06-01\"), (high_2015 + low_2015)/2 - 0.2),\n            fontsize=8, ha='left', color='gray',\n            arrowprops=dict(arrowstyle='->', color='gray', lw=0.8))\n\n# Labels and title (using F-E convention)\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Relative Uplift (m)\")\nax.set_title(\"Differential Uplift at Axial Seamount (MJ03F − MJ03E)\")\n\n# Format x-axis\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\nax.xaxis.set_major_locator(mdates.YearLocator())\n\n# Clean up\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\nax.legend(loc=\"lower right\", framealpha=0.9, fontsize=9)\n\nplt.tight_layout()\n\n# Save figure\nfig.savefig(OUTPUT_DIR / \"figures\" / \"differential_uplift.png\", dpi=150, bbox_inches=\"tight\")\nprint(\"Saved: figures/differential_uplift.png\")\n\nplt.show()\n\n# Reset rcParams\nplt.rcParams.update(plt.rcParamsDefault)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interpretation\n\nThe plot shows the characteristic volcanic cycle at Axial Seamount:\n\n1. **Early 2015**: Elevated differential indicating the caldera center was inflated relative to the rim\n2. **April 24, 2015**: Sharp drop marking the eruption - rapid deflation as magma evacuated the chamber. The differential dropped by ~1.3 m (total deflation at MJ03F was ~2.4 m; the differential is smaller because MJ03E also deflects somewhat)\n3. **2015-2026**: Steady re-inflation as the magma chamber recharges\n4. **Variable inflation rates**: Post-2015 rates ranged from >100 cm/yr (initially) to ~10-15 cm/yr (recent years)\n5. **~2022-2023**: The differential approached the 2015 pre-eruption threshold (~94% re-inflation by March 2023)\n6. **Present (2026)**: Now within or above the threshold uncertainty band\n\n**Forecasting caveat:** While Axial exhibits \"inflation-predictable\" behavior, the exact eruption threshold varies. The 2015 threshold was ~30 cm higher than 2011. As the Axial research team notes: \"the pattern could change.\" The dashed lines represent this uncertainty, not a precise prediction window."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Export\n",
    "\n",
    "Export the cleaned data to Parquet format for use in other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export hourly data\n",
    "combined.to_parquet(OUTPUT_DIR / \"differential_uplift_hourly.parquet\")\n",
    "print(f\"Exported: differential_uplift_hourly.parquet ({len(combined)} rows)\")\n",
    "\n",
    "# Export daily data\n",
    "daily.to_parquet(OUTPUT_DIR / \"differential_uplift_daily.parquet\")\n",
    "print(f\"Exported: differential_uplift_daily.parquet ({len(daily)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Exported Data\n",
    "\n",
    "To use this data in another analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: loading and joining with another dataset\n",
    "bpr = pd.read_parquet(OUTPUT_DIR / \"differential_uplift_daily.parquet\")\n",
    "print(bpr.head())\n",
    "print(f\"\\nColumns: {list(bpr.columns)}\")\n",
    "print(f\"Index: {bpr.index.name} ({bpr.index.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\nThis notebook demonstrated:\n\n1. **Data loading** from OOI NetCDF archives with memory-efficient chunked processing\n2. **Pressure-to-depth conversion** using a simplified linear relationship\n3. **Quality control** using MAD-based spike detection (more robust than standard deviation)\n4. **Differential uplift calculation** (F - E convention) to isolate volcanic deformation from oceanographic signals\n5. **Visualization** of the ~11-year volcanic inflation/deflation cycle with eruption threshold references\n6. **Data export** to Parquet for cross-instrument analysis\n\n### Key Findings\n\n- Axial Seamount has been steadily inflating since the April 2015 eruption\n- As of early 2026, the differential uplift has reached or exceeded the 2015 pre-eruption threshold\n- The inflation rate has been variable: >100 cm/yr initially, slowing to ~10-15 cm/yr in recent years\n- The ±30 cm threshold uncertainty reflects observed variation between the 2011 and 2015 eruption cycles\n\n### Sign Convention Note\n\nThis analysis uses the **F - E convention** (MJ03F minus MJ03E), matching the standard used by the Axial Seamount research team. With this convention:\n- Increasing values = inflation (uplift at caldera center)\n- Decreasing values = deflation (subsidence at caldera center)\n\n### References\n\n- Nooner, S. L., & Chadwick, W. W. (2016). Inflation-predictable behavior and co-eruption deformation at Axial Seamount. *Science*, 354(6318), 1399-1403.\n- Axial Seamount Blog: https://axial.ceoas.oregonstate.edu/axial_blog.html\n- OOI Cabled Array: https://oceanobservatories.org/array/cabled-array/"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}